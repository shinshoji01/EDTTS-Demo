{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "\n",
    "import sys\n",
    "# sys.path.append(\"/home/shoinoue/rsc/projects/sho-meta/pyfiles/\")\n",
    "sys.path.append(\"/Users/shoahoshoaho/Git/sho-meta/pyfiles/\")\n",
    "from json_ext import load_json\n",
    "def text_normalization_dialogtts(text):\n",
    "    text = text.replace(\"[laughter]\", \"\")\n",
    "    replacements = {\n",
    "        \"%\": \"percent\",\n",
    "        \"&\": \"and\",\n",
    "        \"$\": \"dollar\",\n",
    "        \"@\": \"at\",\n",
    "    }\n",
    "    for symbol, word in replacements.items():\n",
    "        text = text.replace(symbol, f\" {word} \")\n",
    "    return text\n",
    "\n",
    "base_dir = \"./audiosamples/\"\n",
    "emo2color = {\n",
    "    \"Angry\": \"red\",\n",
    "    \"Happy\": \"orange\",\n",
    "    \"Neutral\": \"green\",\n",
    "    \"Sad\": \"blue\",\n",
    "}\n",
    "target_emotions = [\"angry\", \"happy\", \"sad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "              # <p>\n",
    "              #   <sup>1</sup>FAIR at Meta<br>\n",
    "              #   <sup>*</sup>Work done at Meta<br>\n",
    "              # </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "webtitle = \"EmoDialogTTS Demo Page\"\n",
    "title = \"EmoDialogTTS: Generating Empathetic Speech Dialogs with Turn-Level Emotion Conditioning\"\n",
    "abstract = \"T\"\n",
    "github_url = \"https://github.com/shinshoji01\"\n",
    "# base_repo_dir = \"/\"\n",
    "base_repo_dir = \"/EDTTS-Demo/\"\n",
    "style_dir = base_repo_dir + \"statics/\"\n",
    "fig_path = base_repo_dir + \"images/inference.png\"\n",
    "\n",
    "            # <h5 class=\"mb-4\">Sho Inoue<sup>1,*</sup>, Min-Jae Hwang<sup>1</sup></h5>\n",
    "initial = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<style>\n",
    "    .red {{ color: red; font-weight: bold; }}\n",
    "    .green {{ color: green; font-weight: bold; }}\n",
    "    .blue {{ color: blue; font-weight: bold; }}\n",
    "    .orange {{ color: orange; font-weight: bold; }}\n",
    "    .grey {{ color: grey; font-weight: bold; }}\n",
    "</style>\n",
    "<html>\n",
    "  <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\">\n",
    "    <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\">\n",
    "    <title>{webtitle}</title>\n",
    "    <link href=\"{style_dir}bootstrap-5.2.3-dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "    <link href=\"{style_dir}my.css\" rel=\"stylesheet\">\n",
    "  </head>\n",
    "  <body>\n",
    "    <div class=\"container\">\n",
    "      <div class=\"row\">\n",
    "        <div class=\"container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded\">\n",
    "          <div class=\"text-center\">\n",
    "            <h2>{title}</h2>\n",
    "            <br>\n",
    "            <h5 class=\"mb-4\">TBD</h5>\n",
    "          </div>\n",
    "          <br>\n",
    "          <figure class=\"text-center\">\n",
    "            <img src=\"{fig_path}\" alt=\"overall diagram of the pipeline\" class=\"img-fluid\" style=\"width: 900px; height: auto;\">\n",
    "          </figure>\n",
    "          <br>\n",
    "          <h3>Abstract</h3>\n",
    "          <p class=\"lead\">\n",
    "          {abstract}\n",
    "          </p>\n",
    "          <p class=\"lead\">You can visit the project page of this paper: <a href=\"{github_url}\">Github Repository</a>.\n",
    "          </p>\n",
    "          <p class=\"lead\">This section presents audio samples produced by our proposed empathetic dialogue-generation system, Emotion-Aware DialogTTS (EDTTS). Each sample demonstrates how different models generate two-speaker conversational audio.\n",
    "          </p>\n",
    "        </div>\n",
    "\"\"\"[1:]\n",
    "\n",
    "closure = f\"\"\"\n",
    "      </div>\n",
    "    </div>\n",
    "    <script src=\"{style_dir}jquery/jquery-3.7.1.slim.min.js\"></script>\n",
    "    <script src=\"{style_dir}bootstrap-5.2.3-dist/bootstrap.min.js\"></script>\n",
    "\"\"\"[1:]\n",
    "closure += \"\"\"\n",
    "  </body>\n",
    "  <script>\n",
    "    $(function(){\n",
    "        $(\"audio\").on(\"play\", function() {\n",
    "            $(\"audio\").not(this).each(function(index, audio) {\n",
    "                audio.pause();\n",
    "                audio.currentTime = 0;\n",
    "            });\n",
    "        });\n",
    "    });\n",
    "    </script>\n",
    "</html>\n",
    "\"\"\"[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    \"syn\": {\n",
    "        \"dialog/prompt\": \"Prompt\",\n",
    "        \"dialog/textmspmanualvad---vad-linear-regression\": \"EmoDialogTTS (ours)\",\n",
    "        # \"dialog/textmspmanualvad_synv3---vad-linear-regression\": \"EDTTS2 (ours)\",\n",
    "        # \"dialog/textmspmanualvad_realfake_synv3---vad-linear-regression\": \"EDTTS3 (ours)\",\n",
    "        \"dialog/baseline---prompt\": \"DialogTTS\",\n",
    "        \"dialog/cosyvoice\": \"CosyVoice\",\n",
    "        \"dialog/mooncast\": \"MoonCast + Speaker Separation\",\n",
    "        \"dialog/mooncast_original\": \"MoonCast\",\n",
    "    },\n",
    "    \"real\": {\n",
    "        \"dialog/prompt\": \"Prompt\",\n",
    "        \"dialog/textmspmanualvad---vad-linear-regression\": \"EmoDialogTTS (ours)\",\n",
    "        # \"dialog/textmspmanualvad_synv3---vad-linear-regression\": \"EDTTS2 (ours)\",\n",
    "        # \"dialog/textmspmanualvad_realfake_synv3---vad-linear-regression\": \"EDTTS3 (ours)\",\n",
    "        \"dialog/baseline---prompt\": \"DialogTTS\",\n",
    "        \"dialog/cosyvoice\": \"CosyVoice\",\n",
    "        \"dialog/mooncast\": \"MoonCast + Speaker Separation\",\n",
    "        \"dialog/mooncast_original\": \"MoonCast\",\n",
    "    },\n",
    "    \"realprompt\": {\n",
    "        \"dialog/prompt\": \"Prompt\",\n",
    "        \"dialog/groundtruth\": \"Ground-Truth\",\n",
    "        \"dialog/textmspmanualvad---vad-linear-regression\": \"EmoDialogTTS (ours)\",\n",
    "        # \"dialog/textmspmanualvad_synv3---vad-linear-regression\": \"EDTTS2 (ours)\",\n",
    "        # \"dialog/textmspmanualvad_realfake_synv3---vad-linear-regression\": \"EDTTS3 (ours)\",\n",
    "        \"dialog/baseline---prompt\": \"DialogTTS\",\n",
    "        \"dialog/cosyvoice\": \"CosyVoice\",\n",
    "        \"dialog/mooncast\": \"MoonCast + Speaker Separation\",\n",
    "        \"dialog/mooncast_original\": \"MoonCast\",\n",
    "    },\n",
    "}\n",
    "\n",
    "vad_base_model = \"intensity/textvad---vad-linear-regression\"\n",
    "# vad_base_model = \"intensity/textvad---prompt\"\n",
    "# vad_base_model = \"intensity/textvad_realfake_synv3---vad-linear-regression\"\n",
    "intensity_list = [\"0.0\", \"0.2\", \"0.4\", \"0.6\", \"0.8\", \"1.0\"]\n",
    "for emotion in target_emotions:\n",
    "    experiments[f\"intensity_{emotion}\"] = {\"intensity/prompt\": \"Prompt\"}\n",
    "    for intkey in intensity_list:\n",
    "        experiments[f\"intensity_{emotion}\"][vad_base_model+f\"___{intkey}\"] = f\"{emotion[0].upper()+emotion[1:]} Intensity {intkey}\"\n",
    "\n",
    "titles = {\n",
    "    \"syn\": \"Empathetic dialog generation (empathetic text dialogs, \\\"SynPrompts\\\")\",\n",
    "    \"real\": \"Empathetic dialog generation (empathetic text dialogs, \\\"RealPrompts\\\")\",\n",
    "    \"realprompt\": \"Speech dialog generation (ground-truth text dialogs, \\\"RealPrompts\\\")\",\n",
    "}\n",
    "for emo in [emo[0].upper()+emo[1:] for emo in target_emotions]:\n",
    "    titles[\"intensity_\"+emo.lower()] = f\"Emotion intensity controllability via valence-arousal-dominance controls (<span class=\\\"{emo2color[emo]}\\\">{emo}</span>)\"\n",
    "\n",
    "explanations = {\n",
    "    \"syn\": \"\\\n",
    "We constructed the empathetic text dialogues using a large language model (LLM). For the speech prompts, we concatenated utterance-level speech samples to simulate two-channel speech dialogues. \\\n",
    "<ul> \\\n",
    "  <li><b>Prompt</b>: The input speech prompts used for inference. To create two-speaker conversations, we combined single-speaker audio clips from the RAVDESS and ESD datasets. <br></li> \\\n",
    "  <li><b>EmoDialogTTS (ours)</b>: Our emotion-aware version of DialogTTS, designed to generate empathetic speech dialogues.<br></li> \\\n",
    "  <li><b>DTTS</b>: The original DialogTTS model, which does not use any emotion information when generating speech.<br></li> \\\n",
    "  <li><b>CosyVoice</b>: Speech generated by CosyVoice. Since CosyVoice produces single-utterance samples, we simply concatenated them to form dialogue samples. For the speech prompts, we used expressive speech samples to generate outputs with the target emotion labels.<br></li> \\\n",
    "  <li><b>MoonCast + Speaker Separation</b>: Speech dialogues generated with MoonCast. We then used a speaker-separation process to split the mixed audio into two separate speaker channels.<br></li> \\\n",
    "  <li><b>MoonCast</b>: Speech dialogues generated with MoonCast. <br></li> \\\n",
    "</ul> \\\n",
    "\",\n",
    "    \"real\": \"\\\n",
    "We constructed the empathetic text dialogues using a large language model (LLM). For the speech prompts, we used real two-channel speech dialogues to more accurately simulate natural conversations. \\\n",
    "<ul> \\\n",
    "  <li><b>Prompt</b>: The input speech prompts used for inference. To create two-speaker conversations, we combined single-speaker audio clips from the RAVDESS and ESD datasets. <br></li> \\\n",
    "  <li><b>EmoDialogTTS (ours)</b>: Our emotion-aware version of DialogTTS, designed to generate empathetic speech dialogues.<br></li> \\\n",
    "  <li><b>DTTS</b>: The original DialogTTS model, which does not use any emotion information when generating speech.<br></li> \\\n",
    "  <li><b>MoonCast + Speaker Separation</b>: Speech dialogues generated with MoonCast. We then used a speaker-separation process to split the mixed audio into two separate speaker channels.<br></li> \\\n",
    "  <li><b>MoonCast</b>: Speech dialogues generated with MoonCast. <br></li> \\\n",
    "</ul> \\\n",
    "\",\n",
    "    \"realprompt\": \"\\\n",
    "We used the ground-truth text dialogues and real two-channel speech prompts, used for direct comparison against the ground-truth. \\\n",
    "<ul> \\\n",
    "  <li><b>Prompt</b>: The input speech prompts used for inference. To create two-speaker conversations, we combined single-speaker audio clips from the RAVDESS and ESD datasets. <br></li> \\\n",
    "  <li><b>Ground-Truth</b>: The original, reference speech samples used as the standard for comparison.<br></li> \\\n",
    "  <li><b>EmoDialogTTS (ours)</b>: Our emotion-aware version of DialogTTS, designed to generate empathetic speech dialogues.<br></li> \\\n",
    "  <li><b>DTTS</b>: The original DialogTTS model, which does not use any emotion information when generating speech.<br></li> \\\n",
    "  <li><b>CosyVoice</b>: Speech generated by CosyVoice. Since CosyVoice produces single-utterance samples, we simply concatenated them to form dialogue samples. <br></li> \\\n",
    "  <li><b>MoonCast + Speaker Separation</b>: Speech dialogues generated with MoonCast. We then used a speaker-separation process to split the mixed audio into two separate speaker channels.<br></li> \\\n",
    "  <li><b>MoonCast</b>: Speech dialogues generated with MoonCast. <br></li> \\\n",
    "</ul> \\\n",
    "\",\n",
    "    \"intensity_angry\": \"\\\n",
    "This section presents speech samples showing a range of emotion intensities. Each sample’s emotional intensity is quantified using valence-arousal-dominance (VAD) values. To characterize each emotion, we first computed an emotion vector by subtracting the VAD values of the neutral state from those of the target emotion. We then varied the magnitude of this vector in its respective direction to generate speech samples with different emotion intensities. \\\n",
    "\",\n",
    "    \"intensity_happy\": \"\\\n",
    "This section presents speech samples showing a range of emotion intensities. Each sample’s emotional intensity is quantified using valence-arousal-dominance (VAD) values. To characterize each emotion, we first computed an emotion vector by subtracting the VAD values of the neutral state from those of the target emotion. We then varied the magnitude of this vector in its respective direction to generate speech samples with different emotion intensities. \\\n",
    "\",\n",
    "    \"intensity_sad\": \"\\\n",
    "This section presents speech samples showing a range of emotion intensities. Each sample’s emotional intensity is quantified using valence-arousal-dominance (VAD) values. To characterize each emotion, we first computed an emotion vector by subtracting the VAD values of the neutral state from those of the target emotion. We then varied the magnitude of this vector in its respective direction to generate speech samples with different emotion intensities. \\\n",
    "\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = glob.glob(base_dir+f\"{base_model}/*/*/*/*.wav\")\n",
    "# filenames = list(set([p[len(base_dir)+len(base_model)+1:] for p in files]))\n",
    "# filenames.sort()\n",
    "# filenames = filenames[::2]\n",
    "# np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_dir = {}\n",
    "base_model = \"dialog/baseline---prompt\"\n",
    "files = glob.glob(base_dir+f\"{base_model}/syn/*/*/*/*.wav\")\n",
    "filenames = list(set([p[len(base_dir)+len(base_model)+1:] for p in files]))\n",
    "filenames.sort()\n",
    "filenames = [[fn.replace(\"/\"+emo+\"/\", \"/*/\") for emo in emo2color if emo in fn][0] for fn in filenames[::2]]\n",
    "np.random.seed(0)\n",
    "filenames = [str(p) for p in list(np.random.choice(filenames, len(filenames), replace=False))]\n",
    "\n",
    "newfiles = []\n",
    "for f, fn in enumerate(filenames):\n",
    "    np.random.seed(f)\n",
    "    newfiles += [str(p)[len(base_dir)+len(base_model)+1:] for p in np.random.choice(glob.glob(base_dir+f\"{base_model}/{fn}\"), 2, replace=False)]\n",
    "filenames_syn = [fn for fn in newfiles if not(\"davidaitest\" in fn)]\n",
    "filenames_real = [fn for fn in newfiles if \"davidaitest\" in fn]\n",
    "\n",
    "filenames_dir[\"syn\"] = filenames_syn\n",
    "filenames_dir[\"real\"] = filenames_real\n",
    "\n",
    "files = glob.glob(base_dir+f\"{base_model}/real/*/*/*/*.wav\")\n",
    "filenames = list(set([p[len(base_dir)+len(base_model)+1:] for p in files]))\n",
    "filenames.sort()\n",
    "np.random.seed(0)\n",
    "filenames = [str(p) for p in np.random.choice(filenames, len(filenames))]\n",
    "\n",
    "filenames_dir[\"realprompt\"] = filenames\n",
    "\n",
    "base_model = vad_base_model + f\"___0.0\"\n",
    "files = glob.glob(base_dir+f\"{base_model}/syn/*/*/*/*.wav\")\n",
    "files.sort()\n",
    "files = [str(file) for file in np.random.choice(files, size=len(files), replace=False)]\n",
    "filenames = [file[len(base_dir)+len(base_model)+1:] for file in files]\n",
    "for emo in [\"angry\", \"happy\", \"sad\"]:\n",
    "    filenames_dir[f\"intensity_{emo}\"] = [fn for fn in filenames if f\"___{emo}.wav\" in fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "extexts = {}\n",
    "for exid in experiments:\n",
    "    text = f\"\"\"\n",
    "            <div class=\"container pt-5 mt-5 shadow p-5 mb-5 bg-white rounded\">\n",
    "              <h3>{titles[exid]}</h3>\n",
    "              <p class=\"lead\">\n",
    "                    {explanations[exid]}\n",
    "                    </p>\n",
    "              <div class=\"table-responsive\" style=\"overflow-x: scroll\">\n",
    "                <table class=\"table table-sm\">\n",
    "    \"\"\"[1:]\n",
    "    text += f\"\"\"\n",
    "                  <thead>\n",
    "                    <tr>\n",
    "                      <th scope=\"col\">ID</th>\n",
    "    \"\"\"[1:]\n",
    "    for fn in filenames_dir[exid]:\n",
    "        if \"intensity\" in exid:\n",
    "            title = \"/\".join(fn.split(\"/\")[1:3])\n",
    "        else:\n",
    "            title = \"/\".join(fn.split(\"/\")[1:4])\n",
    "        text += f\"\"\"\n",
    "                      <th scope=\"col\" class=\"text-center\">{title}</th>\n",
    "    \"\"\"[1:]\n",
    "    text += \"\"\"\n",
    "                    </tr>\n",
    "                  </thead>\n",
    "    \"\"\"[1:]\n",
    "    headerfn = text\n",
    "\n",
    "    text = \"\"\"\n",
    "                  <tbody>\n",
    "                    <tr>\n",
    "                      <th scope=\"row\" style=\"position: sticky; left: 0; z-index:10; opacity: 1.0; background-color: white;\">Text</th>\n",
    "    \"\"\"[1:]\n",
    "    for fn in filenames_dir[exid]:\n",
    "        if \"intensity\" in exid:\n",
    "            # emo = os.path.basename(fn).split(\"___\")[-1:]\n",
    "            # emo = emo[:-4]\n",
    "            # html_table = f\"Intensity: {intkey} ({emo})\"\n",
    "            transcript = load_json(base_dir + f\"intensity/prompt/{fn}\".replace(\".wav\", \".json\"))[\"Turn 1\"][\"transcription\"]\n",
    "            html_table = transcript\n",
    "        else:\n",
    "            html_table = '<table style=\"border-collapse: separate; border-spacing: 0 5px;\">'\n",
    "            key = list(experiments[exid].keys())[0]\n",
    "            dialog = load_json(base_dir + f\"{key}/{fn}\".replace(\".wav\", \".json\"))\n",
    "            for turnkey in dialog:\n",
    "                turn = dialog[turnkey]\n",
    "                emo = turn[\"emotion\"]\n",
    "                spk = turn[\"speaker\"]\n",
    "                spk = spk[0].upper() + spk[1:]\n",
    "\n",
    "                html_table += \"<tr>\"\n",
    "                transcript = text_normalization_dialogtts(turn[\"transcription\"])\n",
    "                if exid in [\"realprompt\"]:\n",
    "                    spk = \"Left\" if spk==\"User\" else \"Right\"\n",
    "                    html_table += (\n",
    "                        f'<td style=\"text-align: center; vertical-align: middle; min-width: 100px;\">'\n",
    "                        f'<span style=\"font-size: 0.8rem;\">'\n",
    "                        f'<strong>{spk}</strong><br>'\n",
    "                        f'</span>'\n",
    "                        f'</td>'\n",
    "                    )\n",
    "                    html_table += f'<td style=\"text-align: center; vertical-align: middle; min-width: 300px;\"><span style=\"font-size: 0.8rem;\">{transcript}</span></td>'\n",
    "                else:\n",
    "                    emocolor = emo2color[emo]\n",
    "                    html_table += (\n",
    "                        f'<td style=\"text-align: center; vertical-align: middle; min-width: 100px;\">'\n",
    "                        f'<span style=\"font-size: 0.8rem;\">'\n",
    "                        f'<strong>{spk}</strong><br>'\n",
    "                        f'(<span class=\"{emocolor}\">{emo}</span>)'\n",
    "                        f'</span>'\n",
    "                        f'</td>'\n",
    "                    )\n",
    "                    html_table += f'<td style=\"text-align: center; vertical-align: middle; min-width: 300px;\"><span style=\"font-size: 0.8rem;\">{transcript}</span></td>'\n",
    "                html_table += \"</tr>\"\n",
    "            html_table += \"</table>\"\n",
    "        text += f\"\"\"\n",
    "                  <td>\n",
    "                    <p>{html_table}</p>\n",
    "                  </td>\n",
    "    \"\"\"[1:]\n",
    "    text += \"\"\"\n",
    "                    </tr>\n",
    "    \"\"\"[1:]\n",
    "    headertext = text\n",
    "\n",
    "    text = \"\"\n",
    "    for key in experiments[exid]:\n",
    "        text += f\"\"\"\n",
    "                    <tr>\n",
    "                      <th scope=\"row\" style=\"position: sticky; left: 0; z-index:10; opacity: 1.0; background-color: white;\">{experiments[exid][key]}</th>\n",
    "    \"\"\"[1:]\n",
    "        for fn in filenames_dir[exid]:\n",
    "            wavfile = base_dir + f\"{key}/{fn}\"\n",
    "            text += f\"\"\"\n",
    "                      <td style=\"text-align: center; vertical-align: middle; min-width: 100px;\">\n",
    "    \"\"\"[1:]\n",
    "            text += f\"\"\"\n",
    "                        <audio controls=\"\" preload=\"none\" style=\"width: 240px\">\n",
    "                          <source src=\"{wavfile}\" type=\"audio/wav\">\n",
    "                        </audio>\n",
    "    \"\"\"[1:]\n",
    "            text += f\"\"\"\n",
    "                      </td>\n",
    "    \"\"\"[1:]\n",
    "        text += \"\"\"\n",
    "                    </tr>\n",
    "    \"\"\"[1:]\n",
    "    body = text\n",
    "\n",
    "    text = \"\"\"\n",
    "                  </tbody>\n",
    "                </table>\n",
    "              </div>\n",
    "              <p class=\"lead\">* please scroll horizontally to explore additional columns in the table.</p>\n",
    "            </div>\n",
    "    \"\"\"[1:]\n",
    "    tableclosure = text\n",
    "    extexts[exid] = headerfn + headertext + body + tableclosure\n",
    "    extexts[exid] = \"\\n\".join([a[4:] for a in extexts[exid].split(\"\\n\")])\n",
    "    \n",
    "wholetext = \"\"\n",
    "wholetext += initial\n",
    "for exid in experiments:\n",
    "    wholetext += extexts[exid]\n",
    "wholetext += closure\n",
    "f = open(\"index.html\", \"w\")\n",
    "f.write(wholetext)\n",
    "f.close()\n",
    "# print(wholetext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
